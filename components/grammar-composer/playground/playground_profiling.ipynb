{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "506533b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "import hashlib\n",
    "\n",
    "from morpheus.utils import token_quality\n",
    "\n",
    "def hash64(value: int) -> int:\n",
    "    \"\"\"Simple 64-bit hash\"\"\"\n",
    "    h = hashlib.md5(value.to_bytes(4, 'little')).digest()\n",
    "    return int.from_bytes(h[:8], 'little')\n",
    "\n",
    "class RuleFingerprint:\n",
    "    # optimize memory usage by avoiding one __dict__ for each instance\n",
    "    __slots__ = ('bloom', 'xor_hash', 'num_unique', 'num_observations')\n",
    "    MAX_DISTINCT_OBSERVATIONS = 10\n",
    "\n",
    "    def __init__(self):\n",
    "        self.bloom = 0      # 64-bit bloom filter\n",
    "        self.xor_hash = 0   # XOR of unique value hashes\n",
    "        self.num_unique = 0      # Unique count\n",
    "        self.num_observations = 0  # Total observations\n",
    "        \n",
    "    def update(self, value: bytes, token_quality_threshold: float = 0.5):\n",
    "        \"\"\"Add 4-byte observation\"\"\"\n",
    "        assert len(value) == 4, \"Value must be 4 bytes\"\n",
    "\n",
    "        if token_quality(value) < token_quality_threshold:\n",
    "            return\n",
    "        \n",
    "        val = struct.unpack('<I', value)[0]\n",
    "        h = hash64(val)\n",
    "        \n",
    "        # Check if potentially new value\n",
    "        bits = (1 << (h % 64)) | (1 << ((h >> 32) % 64))\n",
    "        is_new = (self.bloom & bits) != bits\n",
    "        \n",
    "        if is_new:\n",
    "            self.bloom |= bits\n",
    "            self.xor_hash ^= h\n",
    "            self.num_unique += 1\n",
    "            \n",
    "            # If we exceed the max distinct observations, set count to overflow marker\n",
    "            if self.num_unique > self.MAX_DISTINCT_OBSERVATIONS:\n",
    "                self.num_unique = 0xFFFF  # Overflow marker\n",
    "    \n",
    "    def to_hex(self) -> str:\n",
    "        \"\"\"Get 16-byte fingerprint\"\"\"\n",
    "        if self.num_unique == 0:\n",
    "            return '00000000000000000000000000000000'\n",
    "        if self.num_unique > self.MAX_DISTINCT_OBSERVATIONS:\n",
    "            return 'FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF'\n",
    "        return struct.pack('<QQH', self.xor_hash, self.bloom, self.num_unique)[:16].hex()\n",
    "\n",
    "    def from_hex(self, hex_str: str):\n",
    "        \"\"\"Load from 16-byte hex string\"\"\"\n",
    "        assert len(hex_str) == 32, \"Hex string must be 32 characters\"\n",
    "        data = bytes.fromhex(hex_str)\n",
    "        self.xor_hash, self.bloom, self.num_unique = struct.unpack('<QQH', data)\n",
    "        \n",
    "        if self.num_unique > self.MAX_DISTINCT_OBSERVATIONS:\n",
    "            self.num_unique = 0xFFFF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cfe1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import os\n",
    "import logging\n",
    "from glob import glob\n",
    "from collections import defaultdict\n",
    "\n",
    "from morpheus.grammar import Grammar\n",
    "\n",
    "def seeds_to_prefixes(seeds):\n",
    "    return [s[:4] for s in seeds if len(s) >= 4]\n",
    "\n",
    "clusters = defaultdict(set)\n",
    "\n",
    "for grammar_path in list(glob(\"/shellphish/libs/nautilus/grammars/reference/*.py\"))[:]:\n",
    "    grammar_name = grammar_path.split(\"/\")[-1].replace(\".py\", \"\")\n",
    "    # if \"@CORPUS\" in grammar_name:\n",
    "    #     continue\n",
    "    if grammar_name in [\"TNEF\", \"LZ4\"]:\n",
    "        continue\n",
    "    \n",
    "    logging.info(f\"Processing grammar: {grammar_name}\")\n",
    "    try:\n",
    "        # logging.info(f\"Loading grammar from: {grammar_path}\")\n",
    "        grammar = Grammar.from_file(grammar_path)\n",
    "\n",
    "        all_nts = {rule.nt for rule in grammar.rules if rule.is_composable()} - {\"ANYRULE\"}\n",
    "        # all_nts = {\"START\"}\n",
    "\n",
    "        for nt in all_nts:\n",
    "            # logging.info(f\"Processing non-terminal: {nt}\")\n",
    "            fp = RuleFingerprint()\n",
    "            fp_base64 = RuleFingerprint()\n",
    "            for seed in grammar.seed_iterator(nt=nt, n=100):\n",
    "                prefix = seed[:4]\n",
    "                fp.update(prefix.ljust(4, b'\\x00'))\n",
    "\n",
    "                base64_seed = base64.b64encode(seed)\n",
    "                base64_prefix = base64_seed[:4]\n",
    "                fp_base64.update(base64_prefix.ljust(4, b'\\x00'))\n",
    "\n",
    "                if fp.num_unique > RuleFingerprint.MAX_DISTINCT_OBSERVATIONS and fp_base64.num_unique > RuleFingerprint.MAX_DISTINCT_OBSERVATIONS:\n",
    "                    # logging.info(f\"Exceeded max distinct observations for {grammar_name} {nt}, stopping early.\")\n",
    "                    break\n",
    "            fp = fp.to_hex()\n",
    "            fp_base64 = fp_base64.to_hex()\n",
    "            clusters[fp].add((grammar_name, nt, None))\n",
    "            clusters[fp_base64].add((grammar_name, nt, \"base64\"))\n",
    "    except:\n",
    "        logging.exception(f\"Failed to load grammar: {grammar_name}\")\n",
    "        continue\n",
    "\n",
    "    # logging.info(f\"Processed grammar: {grammar_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4913aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered clusters size: 72.09 KB\n"
     ]
    }
   ],
   "source": [
    "filtered_clusters = dict(clusters)\n",
    "filtered_clusters = {k: list(v) for k, v in filtered_clusters.items()}\n",
    "if \"FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF\" in filtered_clusters:\n",
    "    del filtered_clusters[\"FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF\"]\n",
    "if \"00000000000000000000000000000000\" in filtered_clusters:\n",
    "    del filtered_clusters[\"00000000000000000000000000000000\"]\n",
    "# filtered_clusters = {k: v for k, v in filtered_clusters.items() if len(v) > 1}\n",
    "# filtered_clusters = {k: sorted(v) for k, v in filtered_clusters.items() if len({p[0] for p in v}) > 1}\n",
    "\n",
    "# print the size in megabytes of filtered_clusters in memory\n",
    "import sys\n",
    "size_in_bytes = sys.getsizeof(filtered_clusters)\n",
    "size_in_kilobytes = size_in_bytes / (1024)\n",
    "print(f\"Filtered clusters size: {size_in_kilobytes:.2f} KB\")\n",
    "\n",
    "# write to ./fingerprints.json\n",
    "import json\n",
    "with open(\"../reference_fingerprints.json\", \"w\") as f:\n",
    "    json.dump(filtered_clusters, f, indent=4, sort_keys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f845fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from morpheus.grammar import Grammar\n",
    "grammar = Grammar.from_file(\"/shellphish/libs/nautilus/grammars/reference/IBOOKS.py\")\n",
    "\n",
    "for grammar in grammar.iter_compositions():\n",
    "    print(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c0ac43",
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar_str = \"\"\"\n",
    "######################################################################\n",
    "# Helper Functions\n",
    "######################################################################\n",
    "\n",
    "def artiphishell_base64_encode(data: bytes) -> bytes:\n",
    "    import base64\n",
    "    return base64.b64encode(data)\n",
    "\n",
    "######################################################################\n",
    "# Grammar Rules\n",
    "######################################################################\n",
    "\n",
    "ctx.rule('START', '{BASE64_DATA}')\n",
    "ctx.rule('BASE64_DATA', '{BASE64_DATA_TXT}')\n",
    "ctx.rule('BASE64_DATA', '{BASE64_DATA_HTML}')\n",
    "ctx.rule('BASE64_DATA', '{BASE64_DATA_PNG}')\n",
    "ctx.rule('BASE64_DATA_TXT', b'SGVsbG8gV29ybGQ=')\n",
    "ctx.rule('BASE64_DATA_HTML', b'PGh0bWw+PGJvZHk+SGVsbG8hPC9ib2R5PjwvaHRtbD4=')\n",
    "ctx.rule('BASE64_DATA_PNG', b'iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNk+A8AAQUBAScY42YAAAAASUVORK5CYII=')\n",
    "# ctx.external('ARTIPHISHELL_NON_ENCODED_PNG', 'PNG', 'START')\n",
    "# ctx.script('BASE64_DATA_PNG', ['ARTIPHISHELL_NON_ENCODED_PNG'], lambda data, encode=artiphishell_base64_encode: encode(data))\n",
    "# ctx.external('ARTIPHISHELL_NON_ENCODED_PNG@CORPUS', 'PNG@CORPUS', 'START')\n",
    "# ctx.script('BASE64_DATA_PNG', ['ARTIPHISHELL_NON_ENCODED_PNG@CORPUS'], lambda data, encode=artiphishell_base64_encode: encode(data))\n",
    "\"\"\"\n",
    "\n",
    "from morpheus.grammar import Grammar\n",
    "grammar = Grammar.from_string(grammar_str)\n",
    "list(grammar.seed_iterator(nt='START', n=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299cd4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "for replacements in grammar.iter_composition_replacements(20):\n",
    "    print([(internal_rule.nt, external_grammar, external_nt, encoding) for internal_rule, external_grammar, external_nt, encoding in replacements])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9d3fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import os\n",
    "import logging\n",
    "from glob import glob\n",
    "from collections import defaultdict\n",
    "\n",
    "from morpheus.grammar import Grammar\n",
    "\n",
    "REFERENCE_RULEHASHES = defaultdict(set)\n",
    "\n",
    "for grammar_path in list(glob(\"/shellphish/libs/nautilus/grammars/reference/*.py\"))[:]:\n",
    "    grammar_name = grammar_path.split(\"/\")[-1].replace(\".py\", \"\")\n",
    "    \n",
    "    logging.info(f\"Processing grammar: {grammar_name}\")\n",
    "    try:\n",
    "        # logging.info(f\"Loading grammar from: {grammar_path}\")\n",
    "        grammar = Grammar._from_file(grammar_name, grammar_path)\n",
    "\n",
    "        for rule in grammar.rules:\n",
    "            if not rule.is_composable():\n",
    "                continue\n",
    "\n",
    "            REFERENCE_RULEHASHES[rule.nt].add(rule.hexdigest)\n",
    "    except:\n",
    "        logging.exception(f\"Failed to load grammar: {grammar_name}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7666ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered clusters size: 144.09 KB\n"
     ]
    }
   ],
   "source": [
    "REFERENCE_RULEHASHES = {k: sorted(v) for k, v in REFERENCE_RULEHASHES.items()}\n",
    "import sys\n",
    "size_in_bytes = sys.getsizeof(REFERENCE_RULEHASHES)\n",
    "size_in_kilobytes = size_in_bytes / (1024)\n",
    "print(f\"Filtered clusters size: {size_in_kilobytes:.2f} KB\")\n",
    "\n",
    "# write to ./fingerprints.json\n",
    "import json\n",
    "with open(\"../reference_rulehashes.json\", \"w\") as f:\n",
    "    json.dump(REFERENCE_RULEHASHES, f, indent=4, sort_keys=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
