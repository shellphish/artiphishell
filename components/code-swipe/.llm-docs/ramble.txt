Below is my vocal description of the component:

Alright we're gonna write some docs for a new component which we're adding to the CRS this new component is called code swipe and the general idea of it is that it's going to be a multi stage filtering and prioritization system which will take a large amount of target source code and try to filter it down into a smaller set of potentially interesting or vulnerable blocks of code or functions within that source code as well as prioritizing them so that whatever analysis systems we have will be able to look at the important code first rather than looking at all of the code at once

So first we're going to try to outline the overall design of this component
This component is designed to be modular allowing us to provide multiple types of filtering and prioritization passes which will be applied against the source code

to start of course we have to have the input side of the component. The inputs to the component of course will have to be first off the source code of the target and potentially information about the task so for like a delta mode we have the change and in full mode we have no change so we just have the basis code but we will also probably want to consume some analysis from some of the other components for example will want the function index information from clang indexer or the equivalent for Java as well as potentially some information about the harnesses potentially from quick seed or grammar guy we'll have to look into if those are actually practical so maybe we'll do those later i'm not sure if they actually give us useful information yet. We want information from code ql as well but I think that's about it in terms of components that we can really get stuff from at the moment we'll have to kind of deal with the rest of the stuff ourselves

Then we of course with the code that we have and the index code we will want to turn it into a large list of all the possible points that we want to perform analysis on the simplest way to do this of course is to split it by function slash method but we may also want to consider breaking large functions into smaller chunks although maybe we won't worry about that right away but that's probably something we'll need to add later on and of course when we have commit information or rather when we have changes to the basis those changes will be the highest priority to begin with as that is where vulnerability is most likely will be added now of course the vulnerabilities might also be exposed in the basis and exposed by those changes so we can't completely ignore the basis code but for now we will probably just ignore it when we have a diff as that is the easiest thing to start with and we can try to go back later and maybe add some extra analysis for related functions that are being exposed maybe that's a some kind of pass will do but okay so then you know obviously we need some sort of models to represent these blocks or functions of code that will be passing through our modular filtering system so we'll need some sort of class for that and you know this collection maybe we'll call it a collection system which will retrieve all of that information from our our input data repos and you know set up a way to quickly iterate over them or or maybe even search with different criteria maybe we want to look for certain string matches or something like that like there's a couple utilities that we can add on top of this to make the future passes easier certain things we can expose maybe we'll do some vector embeddings as well on top of these so that would probably be part of this you know like function class that we have or this code block class then that will you know help us later when we're trying to analyze each of these 

Then we need the actual system for performing the filter passes so we need some sort of a class which acts as the actual mechanism for applying the filters to our function classes and this potentially like there's a couple layers here right because we can potentially rule out some things but it's probably better to not rule things out completely but sort things to the top so maybe some sort of way that we can rank each of these rather than just completely filtering them out so maybe we have 22 classes of things right we have a set of positive matches with with different attributes such as 0 we know this is reachable or we know that this has some sort of match against a heuristic that we have and then on the other hand we have things that have had no matches at all that we don't know a reachable and we'll put them in a separate you know pile which we're not throwing out right away but there is some or rather these should be the least important ones cuz we have no good matches on them so again we need this mechanism and this is where I kind of want you to do a little bit of brainstorming here to help kind of design the program structure for this mechanism of ranking and applying these these matches here and maybe we do some sort of waiting you know some heuristics might be super strong like oh there's a command that's been run here so that might be command injection like that's a very strong heuristic whereas something like this is reachable is something we should you know prioritize over unreachable code but they'll probably be a lot of reachable code so it's not gonna be as strong of AA heuristic for us

And then once we have this mechanism we will want to actually create the actual you know classes that implement it so of course we need some sort of base class that has the kind of functionality that we're going to hook into you know it can it can look at each method and apply a match to it somehow like hey we applied a match with this weight and it has maybe a bunch of metadata associated with it that we can also potentially apply extra weights to and this this top class or this base class for the filtering maybe has some other utility methods i'm not entirely sure what that looks like but you can think about it

Then we need the actual implementations of these heuristics so I'm going to list out a couple of the ideas for heuristics that we have right now and we'll have to flush these out as we go but I will just A brief description of each:
- We have potentially some static analysis of reachability from the harnesses this is where we statically look at the calls that are being made and obviously there's going to be some like direct immediate entry points into the source code but that also can continue out so that's our first simple reachability metric
- Then we might have a dynamic reachability i'm not entirely sure How much of this will have when you run it initially but theoretically we should be able to collect some dynamic coverage information Based off of some initial analysis of the harness potentially from quick seed and this might include more paths that we would not find statically especially if we're dealing with things like reflection or other dynamic pathing. So for this we'll be using coverage Lib and we need some sort of source of input seeds which I'm not quite sure where that source is going to be coming from yet so this one might have to wait until that's been more established
- Another easy heuristic is known dangerous functions depending on the language there's gonna be a set of functions or various things like that which we know for sure are probably points of interest that we want to do analysis on potentially we might wait these differently something like system or you know an exact call is probably gonna be weighted pretty highly something like a stir copy or other extremely dangerous C functions are of course hefty weighted highly but there might be some other functions which are less dangerous but still interesting to look at maybe like some sort of file access Or maybe like querying a database so just doing some sort of static method call matching on that using the information that we have about the code block would be good
- on top of that we might have other general keyword searches or other red flags this could also potentially be using the vector embeddings to find similar code the idea of this certain patterns of code can be dangerous like imagine us SQL command that is being built up by string concatenation or some sort of like path concatenation that gets passed into a file read these things might be harder to match with a static string but there might be other keywords or again this vector similarity which we can make use of to filter these more
- something else that we might want to do is get just a general high level analysis of the harness entry points and sort of figure out what the harnesses are intending for us to do like for example if it's if the harness is giving us access to code of a specific plug-in or something that might be very easy to then uprank those functions related to that plugin maybe that's like a source path thing or something this would be sort of like the most of the LM haul here is is going to be focused on the harness itself rather than the functions the harness is calling as that would be more expensive and that's a different path that will do later
- Um another pass and this is probably gonna be one of the the last ones that we do and maybe this one is gonna be extremely limited to what we apply it to is an LM analysis of functions where we're looking for you know like the LM to provide us some structured output where it is trying to detect certain patterns or vulnerabilities that we know exist in these types of targets so it would be looking again as an example for like the path traversal vulnerabilities where we're looking for like a string building up a path that has user controlled data potentially or like you know some kind of command injection or file read I think that this will be most powerful for these kinds of logic bugs rather than memory corruption necessary but again this will probably be expensive so we're going to need to only run it on a limited set of functions you know at a minimum it should only be functions that we think we're reachable rather than running it on every single function we see so there needs to be some sort of ability of our our system here you know of whatever the filtering mechanism is that's running all of these modular filters to to wait until the end to run the more expensive ones so there's there's gonna be an ordering for these heuristics most likely and then potentially some filter on what goes into that heuristic for these actually expensive calls
- There's a few other things that we can throw in here but I'm not entirely sure what they look like one idea is like the naturalness of the code I'm not actually sure if we're allowed to do this or not the idea is to see if the code kind of looks like it belongs there or if it's like strange like a back door might not look like it belongs there right like if there's some kind of comparison to a header that's unexpected that you wouldn't expect to see it all in the code that's like a red flag as well maybe this is part of the LM analysis not entirely sure where this fits in another similar prospect is code smells Where there's a various ways to look for this again this is maybe a little redundant based on the other passes we have so far but I think there's some other people who might be interested in contributing to this

So that's the general idea for the Structure of this filtering and prioritization system we may add more heuristics but these are the ones I can think of for now again the goal and this is probably something that we should state very clearly at the top of the document the goal is to take a large code base with potentially thousands of functions and reduce it down into a prioritized list of maybe 100 functions or or so I mean obviously we might not actually eliminate things but by prioritizing you know our systems going to be looking at those ones first until like each component runs out of its individual budget or whatever one other consideration as well is the speed of this again the the expensive part both monetarily but also time wise is the LM analysis we need to see what our rate limits are so there might be a lot of paralyzation that we should do here in general we should probably be paralyzed in this quite a bit so that should be a part of the design as well is how to paralyze this so that it gets through the code faster once we have stuff up we'll probably do some benchmarking against some known targets to get an idea of how important the parallelization is but we should keep that possibility of parallelization in mind while we are designing the structure of the code

And then of course this whole system is going to output the you know prioritize list now there's a couple of considerations here we might want to do some sort of immediate streaming output for high value like targets like we see hey you know this method looks really sketchy let's get it into the rest of the system as soon as possible so that we can do analysis on it so that might be one consideration but to begin with let's just stick with outputting a sorted list of all of the code that we're interested in and you know give metadata as well that can be useful for further analysis as the other parts of our system look at it so for example we might see oh we triggered on the heuristic for command injection so we should include some information about that in the output as well

So as I see it this is the general overview of the prioritization part of a code swipe here so what I would like you to do with all this information is compile it into a single read me that kind of describes this component in its entirety and what our goals are and what our plans are and maybe like a road map here of of features that we need to implement and we're you know starting from essentially scratch on this component we're using libraries from our CRS utilities to define at least the input models and the output models but probably also to do things like the coverage lib as well as some other useful methods I don't know how much building or running of inputs we're going to do but that's also provided through that as well and again this document is essentially going to be useful for the LLM agents which will be writing most of this code for me so we need to make sure that it's useful in that regard as a part of the context and so a big part of that is we need to do planning now right we need to think about and we'll do a couple passes of refinement on that to determine you know what the best potential structure of this is in terms of you know our our data structures and the like principles of software that we'll be using here and then our plan for our road map so the ordering that we're implementing all of this the individual steps in the road map and they'll each have their own plan essentially and then also we need to have some way to test it so we'll we'll start thinking as well about the types of tests that we want to create potentially but maybe worry less about the testing now obviously we'll do unit testing I believe as as we start to implement things but overall integration testing will probably have to wait until we have more of the code base up here but obviously that's something that should go in the road map and of course continued documentation is something that's very important so you should include instructions to keep updating this it's a living document here this read me file it should allow us to make changes to our road map and also include instructions that we can add other files to this .llm-docs Directory under this code swipe component here and this is where as we add features we should add individual documents for I think each like major stage in the road map you could probably even pre create some of those files just so that we know that they're there and ready to start filling them out the goal of this of course is to provide a continued living Documentation of this component as it's been developed

As a disclaimer I wrote all of this text here through dictation software so there might be strange mistakes which you should just understand that you know it's a dictation mistake refer to our terminology information at the top level docs as well as other information here I think that's about all you need to write these docs so go ahead if you have questions you can ask me but I would like you to get most of this down first before we go into any refinement or cure it q and a