#!/usr/bin/env python3
import logging
import os

from pathlib import Path
from typing import Optional, Any, List
from pathlib import Path


from agentlib import (

    AgentPlanStep,
    SaveLoadObject,
    Field,
    AgentPlanStepAttempt,
)
from .base_agent import BaseAgent


_l = logging.getLogger(__name__)

class DiffAnalyzerOutput(SaveLoadObject):
    """
    This object describes the seed generated.
    - key1: value_description.
    - key1: value_description.
    """
    reachable: str = Field(
        default="No", description="""
        `Yes` or `No` to indicate whether the commit function is reachable or not from harness.
        """, Optional=False
    )

    vulnerable: str = Field(
        default="No", description="""
        `Yes` or `No` to indicate whether the commit function is vulnerable or not.
        """, Optional=False
    )
    generate_input_script: str = Field(
        default="No", description=""""
        The python script to generate input to trigger vulnerability if there is one. 
        If there is not one generated, leave this field as `No`.
        Only fill this field if the sink function is reachable and vulnerable.
        This python script generates the seed input and save it to output.bin.
        Please make sure that the output.bin is generated by running `python3 <script.py>`. 
        If you write functions in the script, please make sure you have  main function to call them in the script.
        COPY the script you generate in the last task here if they do not have any errors.
        """, Optional=False
    )

class DiffAnalyzerAgent(BaseAgent):
    """
    This agent will follow the steps above.
    """
    current_dir = os.path.dirname(os.path.abspath(__file__))
    prompt_dir = os.path.join(current_dir, "prompts")
    _l.debug(f"llm folder is {prompt_dir}")
    system_prompt = os.path.join(prompt_dir, "da.system.j2")
    user_prompt = os.path.join(prompt_dir, "da.user.j2")

    __SYSTEM_PROMPT_TEMPLATE__ = system_prompt
    __USER_PROMPT_TEMPLATE__ = user_prompt
    __LLM_ARGS__ = {"temperature": 0,
                    "max_tokens": 8192}

    harness_code: str
    jazzer_sanitizer_description: List
    fall_back_python_script: Path
    commit_function: str
    call_chains: List[List[str]]
    functions_on_call_chains: List[str]
    previous_script: Optional[str] = None
    
    def get_step_input_vars(self, step: AgentPlanStep) -> dict:
        # Template variables for the prompts
        return dict(
            **super().get_step_input_vars(step),
            harness_code=self.harness_code,
            jazzer_sanitizer_description=self.jazzer_sanitizer_description,
            commit_function=self.commit_function,
            call_chains=self.call_chains,
            functions_on_call_chains=self.functions_on_call_chains,
            previous_script=self.previous_script,
        )

    def on_step_success(self, step: AgentPlanStep, result):
        """
        This is just an example of how you could conditionally skip a step if you wanted.
        """
        remaining_steps = self.plan.steps[self.plan.current_step+1:]
        if step.name == "determine_reachibility":
            # assert isinstance(result, str)
            assert isinstance(result.reachable, str)
            if "no" in result.reachable.lower():
                # Skip over the next step
                # Directly terminate the plan if the LLM think the harness cannot trigger the function. 
                # This will cause the list out of index error. Probably fine
                self.plan.current_step += (len(remaining_steps)-1)
        if step.name == "determine_vulnerability":
            assert isinstance(result.vulnerable, str)
            if "no" in result.vulnerable.lower():
                self.plan.current_step += (len(remaining_steps)-1)

        return super().on_step_success(step, result)

    def validate_step_result(
            self,
            step: AgentPlanStep,
            attempt: AgentPlanStepAttempt,
            result
    ) -> bool:
        # Here we can perform validation on the result of the step
        # If we return False, the agent will retry the step with our feedback

        # This first example will take the llm output and pass it into some other part
        # which uses that output and gives CriticFeedback
        if step.name == 'review_script':
            _l.debug(f"the script is {result}")
            assert (isinstance(result, str))
            res = self.validate_gen_scripts_result(result)
            if res.success:
                return True
            attempt.critic_review = res
            return False
        return super().validate_step_result(step, attempt, result)


class DiffBlockerAgent(DiffAnalyzerAgent):
    def get_available_tools(self):
        return [
            # # Import some predefined tools
            # run_python_script,
            # retrieve_class_source
        ]