#!/usr/bin/env python3
import logging
import os

from pathlib import Path
from typing import Optional, List, Any

from jinja2 import Template

_l = logging.getLogger(__name__)

from agentlib import (
    AgentPlanStepAttempt,
    SaveLoadObject,
    Field,
    AgentPlanStep,
)
from shellphish_crs_utils.function_resolver import FunctionResolver
from shellphish_crs_utils.oss_fuzz.project import OSSFuzzProject
from .base_agent import BaseAgent
from .tools import ToolEnvironment, create_llm_tools, retrieve_java_source, \
run_python_code


class SgOutput(SaveLoadObject):
    """
    This object describes the seed generated.
    - key1: value_description.
    - key1: value_description.
    """
    generate_input_script: str = Field(
        default="No", description=""""
        The python script produced in the last task. This python script generates the seed input and save it to output.bin.
        Please make sure that the output.bin is generated by running `python3 <script.py>`. 
        If you write functions in the script, please make sure you have  main function to call them in the script.
        COPY the script you generate in the last task here if they do not have any errors.
        """, Optional=False
    )



class BlockerAnalyzerAgent(BaseAgent):
    """
    This agent will follow the steps above.
    """
    current_dir = os.path.dirname(os.path.abspath(__file__))
    prompt_dir = os.path.join(current_dir, "prompts")
    _l.debug(f"llm folder is {prompt_dir}")
    system_prompt = os.path.join(prompt_dir, "pa.system.j2")
    user_prompt = os.path.join(prompt_dir, "pa.user.j2")

    __SYSTEM_PROMPT_TEMPLATE__ = system_prompt
    __USER_PROMPT_TEMPLATE__ = user_prompt
    __LLM_ARGS__ = {"temperature": 0,
                    "max_tokens": 8192}
    __RAISE_ON_BUDGET_EXCEPTION__ = True
    __RAISE_ON_RATE_LIMIT_EXCEPTION__ = True
    
    harness_code: str
    source_code: str
    stuck_function_name: str
    stuck_function_src: Optional[str]=None
    next_function_name: Optional[str]=None
    jazzer_sanitizer_description: List
    fall_back_python_script: Path
    next_function_src: Optional[str]=None

    # Requied for the agent tools to run
    harness_name: str
    function_resolver: FunctionResolver
    function_indexer_path: Path
    function_json_dir: Path
    cp_root: Path
    project_source: Path
    fall_back_python_script: Path
    oss_fuzz_build: OSSFuzzProject

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.tool_environment = ToolEnvironment(
            function_indexer_path=self.function_indexer_path,
            function_json_dir=self.function_json_dir,
            project_source=self.project_source,
            cp_root=self.cp_root,
            harness_name=self.harness_name,
            fall_back_python_script=self.fall_back_python_script,
            function_resolver=self.function_resolver,
            oss_fuzz_build=self.oss_fuzz_build,
        )
        create_llm_tools(tool_environment=self.tool_environment)

    def get_step_input_vars(self, step: AgentPlanStep) -> dict:
        # Template variables for the prompts
        return dict(
            **super().get_step_input_vars(step),
            harness_code=self.harness_code,
            source_code=self.source_code,
            stuck_function_name=self.stuck_function_name,
            stuck_function_src=self.stuck_function_src,
            next_function_name=self.next_function_name,
            next_function_src=self.next_function_src,
            jazzer_sanitizer_description=self.jazzer_sanitizer_description,
        )

    def validate_step_result(
            self,
            step: AgentPlanStep,
            attempt: AgentPlanStepAttempt,
            result
    ) -> bool:
        # Here we can perform validation on the result of the step
        # If we return False, the agent will retry the step with our feedback

        # This first example will take the llm output and pass it into some other part which uses that output and gives CriticFeedback
        if step.name == 'generate_script':
            _l.debug(f"the script is {result}")
            assert (isinstance(result, str))
            res = self.validate_gen_scripts_result(result)
            if res.success:
                return True
            attempt.critic_review = res
            return False
        return super().validate_step_result(step, attempt, result)

    def get_available_tools(self):
        return [run_python_code, retrieve_java_source]
