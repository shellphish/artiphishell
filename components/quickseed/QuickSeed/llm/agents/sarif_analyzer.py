#!/usr/bin/env python3
import logging
import os

from pathlib import Path
from typing import Optional, Any, List
from pathlib import Path


from agentlib import (

    AgentPlanStep,
    SaveLoadObject,
    Field,
    AgentPlanStepAttempt,
)
from shellphish_crs_utils.function_resolver import FunctionResolver
from shellphish_crs_utils.oss_fuzz.project import OSSFuzzProject
from .base_agent import BaseAgent
from .tools import ToolEnvironment, create_llm_tools, run_python_code, retrieve_java_source

_l = logging.getLogger(__name__)

class DetermineVulnerabilityOutput(SaveLoadObject):
    """
    This object describes whether the sink function is vulnerable or not
    """
    vulnerable: str = Field(
        default="No", description="""
        `Yes` or `No` to indicate whether the sink function is vulnerable or not.
        """
    )
    reason: Optional[str] = Field(
        default=None, description="""
        The reason why the sink function is vulnerable or not.
        If the sink function is vulnerable, please explain why it is vulnerable.
        If the sink function is not vulnerable, please explain why it is not vulnerable.
        """, Optional=True
    )

class SarifAnalyzerOutput(SaveLoadObject):
    """
    This object describes the seed generated.
    - key1: value_description.
    - key1: value_description.
    """
    reachable: str = Field(
        default="No", description="""
        `Yes` or `No` to indicate whether the sink function is reachable or not from harness.
        """, Optional=False
    )

    vulnerable: str = Field(
        default="No", description="""
        `Yes` or `No` to indicate whether the sink function is vulnerable or not.
        """, Optional=False
    )
    generate_input_script: str = Field(
        default="No", description=""""
        The python script to generate input to trigger vulnerability if there is one. 
        If there is not one generated, leave this field as `No`.
        Only fill this field if the sink function is reachable and vulnerable.
        This python script generates the seed input and save it to output.bin.
        Please make sure that the output.bin is generated by running `python3 <script.py>`. 
        If you write functions in the script, please make sure you have  main function to call them in the script.
        COPY the script you generate in the last task here if they do not have any errors.
        """, Optional=False
    )

class SarifAnalyzerAgent(BaseAgent):
    """
    This agent will follow the steps above.
    """
    current_dir = os.path.dirname(os.path.abspath(__file__))
    prompt_dir = os.path.join(current_dir, "prompts")
    _l.debug(f"llm folder is {prompt_dir}")
    system_prompt = os.path.join(prompt_dir, "sa.system.j2")
    user_prompt = os.path.join(prompt_dir, "sa.user.j2")

    __SYSTEM_PROMPT_TEMPLATE__ = system_prompt
    __USER_PROMPT_TEMPLATE__ = user_prompt
    __LLM_ARGS__ = {"temperature": 0,
                    "max_tokens": 8192}
    __RAISE_ON_BUDGET_EXCEPTION__ = True
    __RAISE_ON_RATE_LIMIT_EXCEPTION__ = True

    harness_code: str
    jazzer_sanitizer_description: List
    fall_back_python_script: Path
    rule_id: str
    message: str
    data_flows: List[List[str]]
    sink_function: str
    source_and_traces: str

    function_indexer_path: Path
    function_json_dir: Path
    project_source: Path
    cp_root: Path
    harness_name: str
    function_resolver: FunctionResolver
    oss_fuzz_build: OSSFuzzProject

    previous_script: Optional[str] = None


    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.tool_environment = ToolEnvironment(
            function_indexer_path=self.function_indexer_path,
            function_json_dir=self.function_json_dir,
            project_source=self.project_source,
            cp_root=self.cp_root,
            harness_name=self.harness_name,
            fall_back_python_script=self.fall_back_python_script,
            function_resolver=self.function_resolver,
            oss_fuzz_build=self.oss_fuzz_build,
        )
        create_llm_tools(tool_environment=self.tool_environment)


    def get_step_input_vars(self, step: AgentPlanStep) -> dict:
        # Template variables for the prompts
        return dict(
            **super().get_step_input_vars(step),
            harness_code=self.harness_code,
            jazzer_sanitizer_description=self.jazzer_sanitizer_description,
            rule_id=self.rule_id,
            message=self.message,
            data_flows=self.data_flows,
            sink_function=self.sink_function,
            previous_script=self.previous_script,
            source_and_traces=self.source_and_traces,
        )

    def on_step_success(self, step: AgentPlanStep, result):
        """
        This is just an example of how you could conditionally skip a step if you wanted.
        """

        remaining_steps = self.plan.steps[self.plan.current_step+1:]
        if step.name == "determine_reachibility":
            # assert isinstance(result, str)
            assert isinstance(result.reachable, str)
            if "no" in result.reachable.lower():
                # Skip over the next step
                # Directly terminate the plan if the LLM think the harness cannot trigger the function. 
                # This will cause the list out of index error. Probably fine
                self.plan.current_step += (len(remaining_steps)-1)
        if step.name == "determine_vulnerability":
            assert isinstance(result.vulnerable, str)
            if "no" in result.vulnerable.lower():
                self.plan.current_step += (len(remaining_steps)-1)

        return super().on_step_success(step, result)

    def validate_step_result(
            self,
            step: AgentPlanStep,
            attempt: AgentPlanStepAttempt,
            result
    ) -> bool:
        # Here we can perform validation on the result of the step
        # If we return False, the agent will retry the step with our feedback

        # This first example will take the llm output and pass it into some other part
        # which uses that output and gives CriticFeedback
        if step.name == 'review_script':
            _l.debug(f"the script is {result}")
            assert (isinstance(result, str))
            res = self.validate_gen_scripts_result(result)
            if res.success:
                return True
            attempt.critic_review = res
            return False
        return super().validate_step_result(step, attempt, result)

    def get_available_tools(self):
        return [
            # Import some predefined tools
            run_python_code,
            retrieve_java_source
        ]