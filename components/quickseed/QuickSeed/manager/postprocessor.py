import logging
import tempfile
from pathlib import Path
from typing import List, Tuple, Optional
import os
import shutil
import json
import hashlib
import yaml
from queue import Empty
from agentlib import SaveLoadObject
from shellphish_crs_utils.models.crs_reports import CrashingInputMetadata
from shellphish_crs_utils.sarif_resolver import SarifResolver
from QuickSeed.llm import BlockerAnalyzer, BlockerAnalyzerTask, SeedGeneratorTask, SarifReportAnalyzer
from QuickSeed.llm.agents import SgOutput, SarifAnalyzerOutput, DiffAnalyzerOutput
from QuickSeed.parser import CoverageAnalysis, CallGraphParser
from QuickSeed.verifier import SeedTriage
from QuickSeed.utils import run_crash_input
from QuickSeed.manager.scheduler import Scheduler
from QuickSeed.data.metadata import QuickSeedHarnessInfo
from QuickSeed.utils import parse_yajta_result, check_file_size, emit_sarif_assesment
from .processor import Processor
from QuickSeed.utils import upload_crash_input_to_analysis_graph, link_seed_to_sarif

from shellphish_crs_utils.oss_fuzz.project import OSSFuzzProject
from shellphish_crs_utils.sarif_resolver import SarifResult
from shellphish_crs_utils.function_resolver import FunctionResolver
from shellphish_crs_utils.oss_fuzz.project import OSSFuzzProject
# from .task import BlockerAnalyzerTask, SeedGeneratorTask

_l = logging.getLogger(__name__)


class PostProcessor(Processor):
    """
    This PostProcessor is called at the end of the seed generation process,
    specifically after the LLM agent has generated the seeds.
    This producess two types of tasks BlockerAnalyzerTask and SeedGeneratorTask:

    1. For the SeedGeneratorTask agent, this takes benign seeds generated by the LLM agent
    and get the coverage information and produce the BlockerAnalyzerTask.

    2. For the ReflectionAnalyzerTask agent, this takes the seeds generated by the LLM agent
    and get the coverage information, complete the call graph and produce the SeedGeneratorTask

    """

    def __init__(
            self,
            call_graph_parser,
            scheduler,
            jazzer_json,
            available_models,
            coverage_build_target: Path,
            harnesses: List[QuickSeedHarnessInfo],
            project_source: Path,
            oss_fuzz_target: OSSFuzzProject,
            function_resolver: FunctionResolver,
            retry=0,
            dynamic_call_graph: Optional[CallGraphParser] = None,
            sarif_resolver: Optional[SarifResolver] = None,
            # benign_seeds_dir: Optional[Path] = None,
    ):
        super().__init__(call_graph_parser, jazzer_json, scheduler, available_models, harnesses, project_source, function_resolver=function_resolver,
                         dynamic_call_graph=dynamic_call_graph)
        # self.benign_seeds_dir = benign_seeds_dir
        self.retry = retry
        self.oss_fuzz_target = oss_fuzz_target
        self.coverage_build_target = coverage_build_target
        self.total_llm_cost = 0
        self.sarif_resolver = sarif_resolver
        
        # super().__init__("SeedGeneratorPostProcessor")

    def operate(self):
        pass

    def process_result_queue(self, attempt: int=2):
        warmup_processed_tasks = 0
        while True:
            if warmup_processed_tasks >= len(self.harnesses):
                _l.debug("Warmup tasks processed, breaking the loop.")
                break
            try:
                # Wait for results with timeout
                priority, success, result = self.scheduler.result_queue.get(timeout=2.0)               
                # if success and result == Scheduler.TERMINATION_SIGNAL:
                with self.scheduler.lock:
                    active_workers = self.scheduler.active_workers
                    queue_size = len(self.scheduler.task_deque)
                    pending_tasks = self.scheduler.pending_tasks
                _l.info(f"Active workers: {active_workers}, Queue size: {queue_size}, Pending tasks: {pending_tasks}. Processing result ...")
                #     _l.info("Received termination signal, stopping result processing.")
                #     break
                _l.debug(f"The success is {success} and result is {result} in result queue")
                
                if not success:
                    _l.error("Task failed, continuing to next result")
                    continue
                task, _ = result
                if task.name == "SeedGeneratorTask":
                    try:
                        process_result, crash_seed_found = self._process_llm_generated_seed(task, result[-1])
                    except Exception as e:
                        _l.error(f"Error processing LLM generated seed for task {task.name}: {e}")
                        continue
                    if not crash_seed_found:
                        generated_seed_path, generated_seed_script = process_result
                    # if generated_seed_path is not None:
                        self.process_seed_generator_result_and_submit_task(task, generated_seed_path, generated_seed_script)

                elif task.name == "ReflectionAnalyzerTask":
                    seeds_output_dir = result[-1][0][-1]
                    query_paths = task.query_paths
                    harness_name = task.harness_name
                    # if not self.coverage_build_target:
                    #     _l.warning(f"Coverage build target is not provided. Skipping reflection analyzer.")
                    #     continue
                    coverage_analysis = CoverageAnalysis(self.coverage_build_target, harness_name, seeds_output_dir, java_tracing_type="yajta")
                    yajta_res = coverage_analysis.trace_coverage()
                    self.dynamic_call_graph.update_edges_by_yajta_coverage(yajta_results=yajta_res)
                
                elif task.name == "BlockerAnalyzerTask":
                    try:
                        process_result, crash_seed_found = self._process_llm_generated_seed(task, result)
                    except Exception as e:
                        _l.error(f"Error processing LLM generated seed for task {task.name}: {e}")
                        continue
                    if not crash_seed_found:
                        generated_seed_path, generated_seed_script = process_result
                        if generated_seed_path is not None:
                            self.process_blocker_analyzer_result_and_submit_task(task, generated_seed_path, generated_seed_script)
                elif task.name == "SarifReportAnalyzerTask" or task.name == "DiffAnalyzerTask":
                    # if not isinstance(result[-1][0], SgOutput):
                    try:
                        vulnerable, reachable = self._process_sarif_report_analyzer_result(task, result)
                    except Exception as e:
                        _l.error(f"Error processing Sarif report analyzer result for task {task.name}: {e}")
                        continue
                    if vulnerable and reachable:
                        triaged_seed_script_list, crash_seed_found = self._process_llm_generated_seed(task, result)
                        if task.attempt > attempt:
                            _l.warning(f"Max attempt reached for {task.name}. Skipping this task.")
                        if (not crash_seed_found) and (task.attempt <= attempt):
                            if not triaged_seed_script_list:
                                task.attempt += 1
                                count = self.rotate_model(task.model)
                                harness = self.get_harness(task.harness_name)
                                if task.name == "SarifReportAnalyzerTask":        
                                    self._submit_sarif_analyzer_task(task, "sarif_report_explore_plans.yaml", 
                                                                SarifAnalyzerOutput, count, harness)
                                else:
                                    self._submit_diff_analyzer_task(task, "diff_analyzer_plans.yaml",
                                                                DiffAnalyzerOutput, count, harness)
                            else:
                                if task.name == "SarifReportAnalyzerTask":
                                    self.process_sarif_analyzer_result_and_submit_task(task, triaged_seed_script_list)
                                else:
                                    self.process_diff_analyzer_result_and_submit_task(task, triaged_seed_script_list)
                elif task.name == "DiffAnalyzerTask":
                    vulnerable, reachable = self._process_sarif_report_analyzer_result(task, result)
                elif task.name == "WarmUpTask":
                    warmup_processed_tasks += 1
                    _l.debug(f"Warmup task processed: {warmup_processed_tasks}/{len(self.harnesses)}")
                    try:
                        self.process_warm_up_task_result(task, result)
                    except Exception as e:
                        _l.error(f"Error processing warm up task result for task {task.name}: {e}")
                        continue
            except Empty:
                # No result available, check if more tasks are pending
                # self.scheduler.debug_worker_status()
                with self.scheduler.lock:
                    active_workers = self.scheduler.active_workers
                    queue_size = len(self.scheduler.task_deque)
                    pending_tasks = self.scheduler.pending_tasks
                if pending_tasks > 0:
                    # DO NOT COMMIT
                    # _l.debug(f"No result yet, but {pending_tasks} tasks still pending, {active_workers} active workers, {queue_size} tasks in queue. Waiting...")
                    continue  # Keep waiting
                else:
                    _l.debug("No pending tasks and no more results. Processing complete.")
                    break

    def _check_seed_type(self, task, result)-> List[Tuple[Path, List[Path]]]:
        """
        Check whether generated seed is crashing seed or benign seed
        """
        _l.debug(f"Checking seed type ...")
        if task.name == "SarifReportAnalyzerTask" or task.name == "DiffAnalyzerTask":
            script_seed_list = result[-1][-1]
        else:
            _, script_seed_list = result
        # harness_filepath = task.harness_filepath
        triaged_script_seed_list = []

        if not script_seed_list:
            return []
        generated_seeds_md5 = []
        for generated_seed_script, generated_seed in script_seed_list:
            generated_seed_paths = []
            if Path(generated_seed).is_file():
                if self.add_if_is_not_triaged(generated_seed, generated_seeds_md5):
                    continue
                generated_seed_paths = [self._generate_alerts(generated_seed, task)]
            else:
                for seed in Path(generated_seed).iterdir():
                    if self.add_if_is_not_triaged(seed, generated_seeds_md5):
                        continue
                    generated_seeds_md5.append(seed)
                    generated_seed_paths.append(self._generate_alerts(seed, task))
            triaged_script_seed_list.append((generated_seed_script, generated_seed_paths))
        return triaged_script_seed_list

    def _process_llm_generated_seed(self, task, result: List[Tuple[Path, Path]]):
        triaged_script_seed_list = self._check_seed_type(task, result)
        if triaged_script_seed_list == []:
            _l.warning(f"‚ùå Seed generation failed for {task}")
            return (None, None), None
        crash_seed_found = False
        harness_benign_seeds_dir, harness_crash_seeds_dir = self.get_harness_benign_crash_seeds_dir(task.harness_name)
        triaged_seeds_md5 = []
        for _, generated_seed_paths in triaged_script_seed_list:
            # if self._is_benign_seed(generated_seed_path):
            #     _l.debug(f"Generated seed is benign seed")
            #     blocker_analyzer_for_benign_seeds.append((task, generated_seed_path, generated_seed_script))
            for generated_seed_path in generated_seed_paths:
                with open(generated_seed_path, "rb") as f:
                    md5name = hashlib.md5(f.read()).hexdigest()
                if md5name in triaged_seeds_md5:
                    continue
                triaged_seeds_md5.append(md5name)
                if self._is_crash_seed(generated_seed_path, harness_crash_seeds_dir):
                    crash_seed_found = True
                    if not check_file_size(generated_seed_path):
                        _l.warning(f"‚ùå Generated seed {generated_seed_path} is too large, skipping it")
                        continue
                    if os.getenv("CRASH_DIR_PASS_TO_POV"):
                        crash_dir = os.getenv("CRASH_DIR_PASS_TO_POV")
                        crash_metadata_dir = os.getenv("CRASH_METADATA_DIR_PASS_TO_POV")
                        seed_file = Path(crash_dir) / md5name
                        seed_meta_file = Path(crash_metadata_dir) / md5name
                        shutil.copy(generated_seed_path, seed_file)

                        harness_name = task.harness_name
                        harness_index = self.harness_names.index(harness_name)
                        harness_data = {
                            k: v for k, v in self.harnesses[harness_index].model_dump().items() 
                            if k in CrashingInputMetadata.model_fields
                        }
                        harness_data = CrashingInputMetadata.model_validate(harness_data)
                        with open(seed_meta_file, "w") as f:
                            yaml.safe_dump(harness_data.model_dump(mode='json'), f, default_flow_style=False, sort_keys=False)
                        if os.getenv("QUICKSEED_CRASHING_SEED_BACKUP"):
                            backup_dir = os.getenv("QUICKSEED_CRASHING_SEED_BACKUP")
                            shutil.copy(generated_seed_path, Path(backup_dir) / md5name)
                    if self.sarif_resolver:
                        _l.debug(f"üì§ Uploading crash input to analysis graph and linking to SARIF report")
                        crash_id = upload_crash_input_to_analysis_graph(harness_data.harness_info_id, harness_data, generated_seed_path, True)
                        sarif_id = task.sarif_meta.pdt_sarif_id
                        link_seed_to_sarif(crash_id, sarif_id, self.sarif_resolver)
                        emit_sarif_assesment(task.sarif_meta)
                    _l.debug(f"‚úÖ Generated seed is crash seed")
        if crash_seed_found:
            _l.debug(f"üü¢ Found crash seed in generated seeds")
            _l.debug(f"We have {len(list(harness_benign_seeds_dir.iterdir()))} seeds in benign seeds dir and {len(list(harness_crash_seeds_dir.iterdir()))} seeds in crash seeds dir")
            return triaged_script_seed_list, crash_seed_found
        if not crash_seed_found:           
            _l.debug(f"üü° Generated seed is benign seed")
        _l.debug(f"We have {len(list(harness_benign_seeds_dir.iterdir()))} seeds in benign seeds dir and {len(list(harness_crash_seeds_dir.iterdir()))} seeds in crash seeds dir")
        if not crash_seed_found:
            if task.name == "SarifReportAnalyzerTask" or task.name == "DiffAnalyzerTask":
                return triaged_script_seed_list, crash_seed_found
            else:
                return (triaged_script_seed_list[0][1][0], triaged_script_seed_list[0][0]), crash_seed_found
        else:
            return (None, None), None

    def _complete_call_graph_with_reflection_analyzer_result(self, task):
        reflection_seeds_dir = task.output_dir
        query_paths = task.query_paths
        harness_name = task.harness_name
        coverage_analysis = self._get_btrace_coverage(reflection_seeds_dir, harness_name)

        # FIXME: complete call graph with reflection analyzer result
        # coverage_analysis.parse_btrace_results_to_complete_graph(self.call_graph_parser, query_paths)


    # FIXME: make it use jyta
    def _get_btrace_coverage(self, reflection_seeds_dir, harness_name):
        coverage_analysis = CoverageAnalysis(self.coverage_build_target, harness_name, reflection_seeds_dir,
                                             java_tracing_type="btrace")
        coverage = coverage_analysis.trace_coverage()
        return coverage_analysis


    def _generate_alerts(self, generated_seed, task):

        # for generated_input in self.benign_seeds_dir.iterdir():
        harness_name = task.harness_name
        harness_path = task.harness_filepath
        harness_benign_seeds_dir, harness_crash_seeds_dir = self.get_harness_benign_crash_seeds_dir(harness_name)
        seed_triage = SeedTriage(self.oss_fuzz_target, harness_name, harness_path, harness_benign_seeds_dir, harness_crash_seeds_dir)

        # exit_type = 0 means that it create a crash seed or a valid benign seed, which is good
        # exit_type = 1 means that it is an invalid seed
        # _l.debug(f"generate seed path is {genreated_seed_path}")
        generated_seed_path = seed_triage.generates_alerts(generated_seed)
        return generated_seed_path

    def _is_benign_seed(self, generated_seed_path, benign_seeds_dir):
        return Path(str(generated_seed_path)).is_relative_to(benign_seeds_dir)
    
    def _is_crash_seed(self, generated_seed_path, crash_seeds_dir):
        return Path(str(generated_seed_path)).is_relative_to(crash_seeds_dir)


    def _submit_blocker_analyzer_task(self, seed_generator_task, generated_seed_script, count, stuck_method_index, attempt):
        node_path = seed_generator_task.node_path
        harness_name = seed_generator_task.harness_name
        harness_filepath = seed_generator_task.harness_filepath
        if stuck_method_index is None:
            stuck_method_index = len(node_path) - 1
        _l.debug(f"At path {node_path}, we are stuck at the function {stuck_method_index}: {node_path[stuck_method_index].function_name}")
        no_harness_path = self.call_graph_parser.cut_harness_from_path(node_path)
        source_code = self.get_source_code(no_harness_path)
        model = self.available_models[count % len(self.available_models)]
        blocker_analyzer_task = BlockerAnalyzerTask(
            node_path=node_path,
            stuck_method_index=stuck_method_index,
            harness_name=harness_name,
            harness_filepath=harness_filepath,
            script_path=generated_seed_script,
            project_source=self.project_source,
            source_code=source_code,
            model=model,
            attempt=attempt,
            jazzer_sanitizer_description=self.jazzer_sanitizer_description
        )
        fallback_seed_gen_scripts_dir = Path(tempfile.mkdtemp())
        agent_plan = tempfile.NamedTemporaryFile(delete=False).name
        harness_benign_seeds_dir, _ = self.get_harness_benign_crash_seeds_dir(harness_name)
        blocker_analyzer = BlockerAnalyzer(
            agent_plan,
            self.call_graph_parser.cp_root,
            self.call_graph_parser.func_indexer_path,
            self.call_graph_parser.function_json_dir,
            model,
            benign_seeds_dir=harness_benign_seeds_dir,
            fall_back_python_script=fallback_seed_gen_scripts_dir,
            function_resolver=self.function_resolver,
            oss_fuzz_build=self.oss_fuzz_target,
        )
        self.scheduler.submit_task_prioritize(blocker_analyzer, blocker_analyzer_task)


    def _submit_blocker_analyzer_task_direct(self, blocker_analyzer_task, generated_seed_script,
                                             count, stuck_method_index, attempt):
        
        # Submit blocker analyzer after a blocker analyzer task still not generating crash seed
        node_path = blocker_analyzer_task.node_path
        harness_name = blocker_analyzer_task.harness_name
        harness_filepath = blocker_analyzer_task.harness_filepath
        model = self.available_models[count % len(self.available_models)]
        if stuck_method_index is None:
            stuck_method_index = len(node_path) - 1
        blocker_analyzer_task = BlockerAnalyzerTask(
            node_path=node_path,
            stuck_method_index=stuck_method_index,
            harness_name=harness_name,
            harness_filepath=harness_filepath,
            script_path=generated_seed_script,
            source_code=blocker_analyzer_task.source_code,
            project_source=self.project_source,
            model=model,
            attempt=attempt,
            jazzer_sanitizer_description=self.jazzer_sanitizer_description,
        )
        harness_benign_seeds_dir, _ = self.get_harness_benign_crash_seeds_dir(harness_name)
        agent_plan = tempfile.NamedTemporaryFile(delete=False).name
        fallback_seed_gen_scripts_dir = Path(tempfile.mkdtemp())
        blocker_analyzer = BlockerAnalyzer(
            agent_plan,
            self.call_graph_parser.cp_root,
            self.call_graph_parser.func_indexer_path,
            self.call_graph_parser.function_json_dir,
            model,
            benign_seeds_dir=harness_benign_seeds_dir,
            fall_back_python_script=fallback_seed_gen_scripts_dir,
            function_resolver=self.function_resolver,
            oss_fuzz_build=self.oss_fuzz_target,
        )
        self.scheduler.submit_task(blocker_analyzer, blocker_analyzer_task)
    
    def process_sarif_analyzer_result_and_submit_task(self, sarif_analyzer_task, triaged_seed_script_list: List[Tuple[Path, List[Path]]]):
        vulnerable_functions_locs = sarif_analyzer_task.sarif_report_result.locations
        # vulnerable_functions_full_names = [self.get_full_method_name_from_sarif_location(loc) for loc in vulnerable_functions_locs]
        path = sarif_analyzer_task.node_path
        functions_on_path = [function.qualified_name for function in path]

        all_covered_functions = []

        for generated_seed_script, generated_seed_paths in triaged_seed_script_list:
            covered_functions = []
            coverage_results = self.yajta_trace(sarif_analyzer_task.harness_name, generated_seed_paths)
            if not coverage_results:
                _l.warning(f"‚ùå Yajta trace failed for {sarif_analyzer_task.harness_name} with generated seed {generated_seed_script}. No coverage results.")
                continue
            covered_functions = self.get_covered_functions_from_yajta_results(coverage_results, functions_on_path)
            all_covered_functions.append(covered_functions)
        
        count = self.rotate_model(sarif_analyzer_task.model) # Rotate the model
        harness = self.get_harness(sarif_analyzer_task.harness_name)
        # agent_plan = tempfile.NamedTemporaryFile(delete=False).name
        with open(triaged_seed_script_list[0][0], "r") as f:
            script = f.read()
        sarif_analyzer_task.attempt += 1
        sarif_analyzer_task.model = self.available_models[count % len(self.available_models)]

        self._submit_sarif_analyzer_task( sarif_analyzer_task, "sarif_blocker_plans.yaml", 
                                         SarifAnalyzerOutput, count, harness, 
                                         covered_functions=all_covered_functions[0],
                                         previous_script=script)
              
    def process_diff_analyzer_result_and_submit_task(self, diff_analyzer_task, triaged_seed_script_list: List[Tuple[Path, List[Path]]]):
        call_chains = diff_analyzer_task.call_chains
        all_functions = []
        for call_chain in call_chains:
            for function in call_chain:
                if function not in all_functions:
                    all_functions.append(function)
        all_covered_functions = []
        for generated_seed_script, generated_seed_paths in triaged_seed_script_list:
            covered_functions = []
            coverage_results = self.yajta_trace(diff_analyzer_task.harness_name, generated_seed_paths)
            if not coverage_results:
                _l.warning(f"‚ùå Yajta trace failed for {diff_analyzer_task.harness_name} with generated seed {generated_seed_script}. No coverage results.")
                continue
            covered_functions = self.get_covered_functions_from_yajta_results(coverage_results, all_functions)
            all_covered_functions.extend([covered_funciton for covered_funciton in covered_functions if covered_funciton not in all_covered_functions])
        
        count = self.rotate_model(diff_analyzer_task.model) # Rotate the model
        harness = self.get_harness(diff_analyzer_task.harness_name)
        # agent_plan = tempfile.NamedTemporaryFile(delete=False).name
        with open(triaged_seed_script_list[0][0], "r") as f:
            script = f.read()
        diff_analyzer_task.attempt += 1
        diff_analyzer_task.model = self.available_models[count % len(self.available_models)]

        self._submit_diff_analyzer_task(diff_analyzer_task, "diff_blocker_analyzer.yaml",
                                        DiffAnalyzerOutput, count, harness,
                                        covered_functions=all_covered_functions[0],
                                        previous_script=script)


    def process_seed_generator_result_and_submit_task(self, seed_generator_task, generated_seed_path, generated_seed_script):
        _l.debug(f"Processing seed generator result for {seed_generator_task.node_path}")
        if generated_seed_path is None:
            _l.warning(f"‚ùå Seed generation failed for {seed_generator_task.node_path}")
            if len(seed_generator_task.left_harnesses_to_try) > 0:

                self.submit_seed_generator_task_to_new_harness(seed_generator_task)
            return
        used_model = seed_generator_task.model
        if used_model in  self.available_models:
            # Rotate the model
            used_model_ind = self.available_models.index(used_model)
            count = (used_model_ind+1) % len(self.available_models)
        else:
            _l.warning(f"‚ùå Model {used_model} is not in the available models list. Using the first model instead.")
            count = 0
        harness_name = seed_generator_task.harness_name     
        coverage_analysis = CoverageAnalysis(self.coverage_build_target, harness_name, generated_seed_path)
        coverage_result = coverage_analysis.trace_coverage()
        if not coverage_result:
            _l.warning(f"‚ùå Coverage analysis failed for {seed_generator_task.node_path}. No coverage result.")
            self.submit_seed_generator_task_to_new_harness(seed_generator_task)
            return
        stuck_method_index = coverage_analysis.parse_jacoco_result_to_find_stuck_method(
            coverage_result, seed_generator_task.node_path
        )
        # TODO: Use yajta to get the full call trace to update our call graph and analysis graph
        # stuck_method_index = -1 means there is no method is triggered, which probably means either our path has problem or 
        # LLM does not have ability to generate seed to even trigger any method
        # stuck_method_index = None means all the method alreay triggered, we can leave it to fuzzer to trigger the crash
        if stuck_method_index is None:
            if seed_generator_task.node_path[-1].qualified_name.startswith("java"):
                _l.warning(f"All methods are triggered for {seed_generator_task.node_path}. We can leave it to fuzzer to trigger the crash.")
                return
            else:
                _l.warning(f"All methods are triggered for {seed_generator_task.node_path}. However, the last method is not java method, continue to blocker task.")
        # This means the generated input does not trigger the first function. It can be two scenarios:
        # 1. The functions are not triggerable from the harness
        # 2. It is triggerable from the harness but it is too hard to one shot generating one input
        if stuck_method_index == -1:
            if len(seed_generator_task.left_harnesses_to_try) == 0:
                # It is triggerable from some harness. It is just that we cannot find it. We won't explore the path any further
                _l.warning(f"We already went through all harnesses and no seeds generated that actually trigger any of the method. Skipping this path.")
            else:
                # This means is not tirggerable from this harness. We should try a new one.

                self.submit_seed_generator_task_to_new_harness(seed_generator_task)
            return
        # TODO: We should add a check to deal with the case where function code of stuck method is None
        attempt = 0
        self._submit_blocker_analyzer_task(seed_generator_task, generated_seed_script, count, stuck_method_index, attempt)

    def process_blocker_analyzer_result_and_submit_task(self, blocker_analyzer_task, generated_seed_path, generated_seed_script):
        _l.debug(f"Processing blocker analyzer result for {blocker_analyzer_task.node_path}")
        used_model = blocker_analyzer_task.model
        used_model_ind = self.available_models.index(used_model)
        attempt = blocker_analyzer_task.attempt
        if attempt >= self.retry:
            return
        count = (used_model_ind + 1) % len(self.available_models)  # Rotate the model
        harness_name = blocker_analyzer_task.harness_name
        coverage_analysis = CoverageAnalysis(self.coverage_build_target, harness_name, generated_seed_path)
        coverage_result = coverage_analysis.trace_coverage()  
        if not coverage_result:
            _l.warning(f"‚ùå Coverage analysis failed for {blocker_analyzer_task.node_path}. No coverage result.")
            return 
        stuck_method_index = coverage_analysis.parse_jacoco_result_to_find_stuck_method(
            coverage_result, blocker_analyzer_task.node_path
        )
        if stuck_method_index is None:
            if blocker_analyzer_task.node_path[-1].qualified_name.startswith("java"):
                _l.warning(f"All methods are triggered for {blocker_analyzer_task.node_path}. We can leave it to fuzzer to trigger the crash.")
                return
            else:
                _l.warning(f"All methods are triggered for {blocker_analyzer_task.node_path}. However, the last method is not java method, continue to blocker task.")
        attempt += 1
        self._submit_blocker_analyzer_task_direct(blocker_analyzer_task, generated_seed_script,
                                                    count, stuck_method_index, attempt)
        
    def submit_seed_generator_task_to_new_harness(self, seed_generator_task):
        left_harnesses_to_try = seed_generator_task.left_harnesses_to_try
        if left_harnesses_to_try is None:
            raise ValueError("left harnesses to try is None")
        tried_harness_index = self.harness_names.index(seed_generator_task.harness_name)

        left_harnesses_to_try.remove(tried_harness_index)
        if  len(left_harnesses_to_try) == 0:
            return
        next_harness_index = left_harnesses_to_try[0]
        harness_name = self.harness_names[next_harness_index]
        _l.debug(f"Submitting seed generator task to new harness {harness_name}")
        seed_generator_task.harness_name = harness_name
        seed_generator_task.harness_filepath = self.harnesses[next_harness_index].harness_source_path
        seed_generator_task.left_harnesses_to_try = left_harnesses_to_try
        with open(seed_generator_task.harness_filepath, "r") as f:
            seed_generator_task.harness_code = f.read()

        self._submit_seed_generator_task(seed_generator_task, seed_generator_task.model)

    def _process_sarif_report_analyzer_result(self, task, result):
        """
        Process the result of the SarifReportAnalyzerTask and submit the next task.
        """
        _l.debug(f"Processing SarifReportAnalyzerTask result")
        agent_result, _ = result[-1]
        if not agent_result:
            _l.warning(f"‚ùå No agent result found in the result of SarifReportAnalyzerTask for {task.harness_name}.")
            return False, False
        vulnerable = False
        reachable = False
        if "yes" == agent_result.vulnerable.lower():
            vulnerable = True
        if "yes" == agent_result.reachable.lower():
            reachable = True

        return vulnerable, reachable

    def yajta_trace(self, harness_name, generated_seed_paths):
        coverage_analysis = CoverageAnalysis(self.coverage_build_target, harness_name, generated_seed_paths, java_tracing_type="yajta")
        coverage_results = []
        coverage_results = coverage_analysis.trace_coverage()
        return coverage_results

    def get_covered_functions_from_yajta_results(self, coverage_results, full_function_name_list):
        covered_functions = []
        for coverage_result in coverage_results:
            visited, edges = parse_yajta_result(coverage_result.get("children"))
            visited_functions_full_names = [self.get_full_method_name_from_yajta_sig(func) for func in visited]
            for full_function_name in full_function_name_list:
                if full_function_name in visited_functions_full_names and full_function_name not in covered_functions:
                    covered_functions.append(full_function_name)
        return covered_functions
    
    def process_warm_up_task_result(self, task, result):
        """
        Process the result of the WarmUpTask and submit the next task.
        """
        _l.debug(f"Processing WarmUpTask result")
        script_seed_list = result[-1]
        if not script_seed_list:
            _l.warning(f"‚ùå No script seed list found in the result of WarmUpTask for {task.harness_name}.")
            return
        for script, generated_path in script_seed_list:
            if generated_path and Path(generated_path).exists():
                covreage_results = self.yajta_trace(task.harness_name, generated_path)
                if not covreage_results:
                    _l.warning(f"‚ùå Yajta trace failed for {task.harness_name} with generated seed {script}. No coverage results.")
                    continue
                self.update_dynamic_call_graph(task.harness_name, covreage_results)

    def update_dynamic_call_graph(self, harness_name, yajta_coverage_results):
        """
        Update the dynamic call graph with individual harness name to reduce false positive
        """
        if yajta_coverage_results:
            self.dynamic_call_graph.update_edges_by_yajta_coverage(
                harness_name=harness_name, 
                yajta_results=yajta_coverage_results
                )