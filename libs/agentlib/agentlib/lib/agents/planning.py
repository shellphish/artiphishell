import os
import json
import time
import subprocess
from typing import Union, Optional, Any, Dict, List, Generic
from pathlib import Path

from langchain.tools import BaseTool
from langchain_core.runnables import RunnableConfig
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.runnables.utils import Input, Output
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.pydantic_v1 import BaseModel, Field, validator
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.output_parsers import BaseOutputParser
from langchain.storage import LocalFileStore
from pydantic import Extra

from .critic import Critic, CriticReview
from .agent import Agent, ChildAgent, AgentResponse, PlainTextOutputParser
from ..common.base import BaseObject
from ..common.object import SaveLoadObject, SaveLoadWithId, LocalObject
from ..common.store import LocalObjectRepository
from ..tools import SerializedTool

class AgentPlanRepository(LocalObjectRepository):
    __ROOT_DIR__ = Path('volumes/agent_plans').resolve()

class AgentPlanStepAttempt(Generic[Input], LocalObject):
    """
    This represents a single attempt to complete a step in a plan.
    This might include multiple messages via tool calls
    """
    step_id: Optional[str] = None
    conversation: list = []
    critic_review: Optional[CriticReview] = None
    """ Review of the finished attempt by the critic """
    result: Optional[Input] = None
    """ Final structured result of the step attempt """

class AgentPlanStep(LocalObject):
    __REPO__ = AgentPlanRepository('steps')

    # === Config ===

    description: str
    """ Description of step goal """

    name: Optional[str] = None
    """ Name of the step, used to identify when a specific step is being acted on in overriding agent method"""

    output_parser: Optional[BaseOutputParser] = None
    """Output parser to apply to the result of this step"""

    available_tools: Optional[List[SerializedTool]] = None
    """Extra tools which can be used during this step"""

    llm_model: Optional[str] = None
    """Override the LLM model for this step"""

    context: Optional[str] = None
    """Additional context for the step, often generated by a separate agent"""

    max_attempts: Optional[int] = None
    """ Max number of failed attempts before we give up on this step """

    do_extract_context: Optional[bool] = True
    """ Whether to extract result context after the step """

    do_critic_review: Optional[bool] = True
    """ Whether to have the critic review the result (only for critical plan executors) """

    critic_instructions: Optional[str] = None
    """ Specific instructions for the critic to use when reviewing this step """

    # === State ===

    attempts: List[AgentPlanStepAttempt] = []
    """ List of attempts so far to complete this step """

    completed: Optional[int] = None
    """ Timestamp of when the step was completed or None if not yet completed"""

    result: Optional[Any] = None
    """ Final result of the step """

    final_context: Optional[str] = None
    """ Unstructured information about what was done in this step """


    def reset(self):
        self.completed = None
        self.result = None
        self.final_context = None
        self.attempts = []

    def get_conversation(self, last=0):
        output = []
        for attempt in self.attempts[-last:]:
            output.extend(attempt.conversation)
        return output

    def get_current_attempt(self):
        if not self.attempts:
            return None
        return self.attempts[-1]

    def new_attempt(self):
        res = AgentPlanStepAttempt(step_id=self.get_id())
        self.attempts.append(res)
        return res

    def check_if_complete(self):
        if self.completed is None:
            if not self.attempts or len(self.attempts) == 0:
                return False
            review = self.attempts[-1].critic_review
            if not review or review.is_success():
                self.completed = int(time.time())
                self.result = self.attempts[-1].result
        return bool(self.completed)

class AgentPlan(LocalObject):
    __REPO__ = AgentPlanRepository('plans')

    steps: List[AgentPlanStep]
    current_step: int = 0

    def get_current_step(self):
        if self.current_step >= len(self.steps):
            return None
        return self.steps[self.current_step]

    def get_past_steps(self):
        return self.steps[:self.current_step]

    def complete_step(self):
        self.current_step += 1

    def is_complete(self):
        return self.current_step >= len(self.steps)

    def save(self):
        for step in self.steps:
            step.save()
        super().save()

    def reset_to_step(self, index):
        self.current_step = index
        for step in self.steps[index:]:
            step.reset()

    def sync_steps(self, steps: List[AgentPlanStep]):
        for i, vs in enumerate(zip(self.steps, steps)):
            v1, v2 = vs
            v1.description = v2.description
            v1.llm_model = v2.llm_model
            v1.output_parser = v2.output_parser
            v1.available_tools = v2.available_tools
            v1.max_attempts = v2.max_attempts
            v1.name = v2.name

        if len(steps) > len(self.steps):
            self.steps.extend(steps[len(self.steps):])
        elif len(steps) < len(self.steps):
            self.steps = self.steps[:len(steps)]
        self.save()


class Planner(ChildAgent[Input, AgentPlan]):
    __OUTPUT_PARSER__ = JsonOutputParser
    __SYSTEM_PROMPT_TEMPLATE__ = 'planner/generic_task_decomposition.system.j2'
    __USER_PROMPT_TEMPLATE__ = 'planner/generic_task_decomposition.user.j2'

    __PLAN_STEP_CLASS__: AgentPlanStep = AgentPlanStep

    def get_env(self) -> Dict[str, str]:
        return {
            **self.get_parent().get_env()
        }

    def get_user_prompt(self):
        return super().get_user_prompt(default='--- Task Context ---\n{env_md}\n--- End Task Context ---\n\n# Final Goal\n{goal}')

    def parse_result(self, output: Any) -> AgentPlan:
        if type(output) is list:
            return AgentPlan(steps=[
                self.__PLAN_STEP_CLASS__(description=x)
                for x in output
            ])
        raise ValueError(f'Invalid planner output: {output}')

    def invoke_agent(self, goal: Input, **kwargs: Any) -> AgentPlan:
        if type(goal) is str:
            goal = dict(goal=goal)
        self.debug(f"Planner invoked with goal {goal}")

        kwargs['config'] = kwargs.get('config', self.runnable_config)
        output = super().invoke_agent(goal, **kwargs)
        if not output.is_success():
            raise ValueError(f'Planner failed to generate a plan: {output}')
        output = output.value
        self.debug(f"Planner output:\n{json.dumps(output, indent=2)}")

        return self.parse_result(output)
    

class PlanExecutor(Generic[Input,Output], Agent[Input, str]):
    __PLANNER_CLASS__: Any = Planner
    __MAX_STEP_ATTEMPTS__ = 10
    __CONTEXT_EXTRACTOR_PROMPT__ = None
    __FINAL_RESULTS_EXTRACTOR_PROMPT__ = None
    __FINAL_RESULTS_PARSER__ = PlainTextOutputParser
    __ATTEMPT_HISTORY_LENGTH__ = 3

    __USE_PROMPT_CACHE_IF_POSSIBLE__ = True

    goal: Optional[Input] = None
    plan: Optional[AgentPlan] = None
    final_results: Optional[Any] = None

    original_input: Optional[Dict[str, Any]] = {}

    # Override this just to control the return type hint
    def invoke(self, input:Input=None, config=None, **kwargs: Any) -> Output:
        return super().invoke(input, config=config, **kwargs)

    @classmethod
    def reload_id_from_file_or_new(cls, filepath, *args, force_json=True, sync_plan=True, **kwargs):
        inst = super().reload_id_from_file_or_new(filepath, *args, force_json=force_json, **kwargs)
        plan = kwargs.get('plan')
        if plan and sync_plan:
            inst.plan.sync_steps(plan.steps)
            inst.save()
        return inst


    def get_final_results_parser(self):
        return self.get_runnable(self.__FINAL_RESULTS_PARSER__)

    def get_planner_class(self) -> Planner:
        return self.__PLANNER_CLASS__

    def on_step_failure(self, step: AgentPlanStep, res: AgentResponse):
        self.warn(f'Plan step failed: {step.description} with result: {res}')

    def on_step_end(self, step: AgentPlanStep, res: AgentResponse):
        pass

    def on_step_success(self, step: AgentPlanStep, res: AgentResponse):
        self.info(f'Plan step succeeded: {step.description} with result: {res}')
        self.trigger_next_step()
        


    def execute_step_attempt(
            self,
            step: AgentPlanStep,
            attempt: AgentPlanStepAttempt,
            **kw
    ) -> AgentResponse[Output]:
        """
        Have the LLM attempt to perform the task.
        This function should return the final result of the attempt.
        """
        res = self.get_agent_response(
            step, 
            output_parser = step.output_parser,
            **kw
        )
        if res:
            attempt.result = res.value
            attempt.conversation = res.chat_messages
        return res

    def validate_step_result(
            self,
            step: AgentPlanStep,
            attempt: AgentPlanStepAttempt,
            result: Any
    ) -> bool:
        """
        Can be overridden to validate or mutate the response
        """
        return True

    def find_all_available_tools(self) -> List[Union[BaseTool, SerializedTool]]:
        cur_step = self.get_current_step()
        return cur_step.available_tools or []

    ####### Internal Methods ########
    
    def get_current_llm(self, output_parser: Optional[BaseOutputParser] = None):
        cur_step = self.get_current_step()
        if cur_step.llm_model:
            llm = self.get_llm_by_name(
                cur_step.llm_model,
                **self.__LLM_ARGS__,
                raise_on_budget_exception=self.__RAISE_ON_BUDGET_EXCEPTION__,
                raise_on_rate_limit_exception=self.__RAISE_ON_RATE_LIMIT_EXCEPTION__
            )
            llm = self.get_llm_for_output_parser(llm, output_parser=output_parser)
            return llm
        return super().get_current_llm()

    def generate_plan(self, goal:str) -> AgentPlan:
        """Use the planner agent to generate a plan for the given goal"""
        planner_cls = self.get_planner_class()
        assert(issubclass(planner_cls, Planner))

        planner: Planner = planner_cls(parent=self)
        return planner.invoke(goal)

    def reset_current_step(self, step: AgentPlanStep):
        step.reset()
        return True

    def resume(self):
        self.ensure_has_plan()
        cur_step = self.plan.get_current_step()
        return self.execute_plan()

    def set_current_goal(self, goal):
        if self.goal:
            if not goal or goal == self.goal:
                self.add_annotation(
                    name='set_plan_goal',
                    text=f'Goal: `{self.goal}`',
                    severity='task',
                )
                return
            raise ValueError(f'Goal already set to {self.goal}, cannot change to {goal}')
        self.goal = goal
        self.plan: AgentPlan = None
        self.add_annotation(
            name='set_plan_goal',
            text=f'Goal: `{goal}`',
            severity='task',
        )

    def ensure_has_plan(self):
        if self.plan is None:
            self.plan = self.generate_plan(self.goal)
            self.debug(f'Generated plan: {self.plan}')

    def trigger_next_step(self):
        self.plan.complete_step()
        self.save()


    def execute_plan(self) -> Output:
        if not self.plan:
            raise ValueError(f'No plan found')

        prev_step_ind = None

        # Step through the plan
        while not self.plan.is_complete():
            step_ind = self.plan.current_step
            step = self.plan.get_current_step()

            if step.completed:
                # Skip completed steps
                self.trigger_next_step()
                continue

            if step.attempts and len(step.attempts) > 0:
                # Resume from the middle of a step
                prev_step_ind = step_ind

            if not step:
                self.warn(f'No step found at step #{self.plan.current_step} (Plan already finished?)')
                break

            # If the current step has changed, reset the step env
            if (
                prev_step_ind is None
                or prev_step_ind != step_ind
            ):
                prev_step_ind = step_ind
                self.reset_current_step(step)


            self.plan.save()
            self.add_annotation(
                name=f'task_step: {step.name or ""}',
                text=f'Starting task: `{step.description}`',
                severity='task',
                extra=dict(
                    plan_id = self.plan.get_id(),
                    step_index = self.plan.current_step,
                )
            )


            was_ok = self.execute_plan_step(step)
            step.check_if_complete()
            res = step.result


            self.on_step_end(step, res)
            if was_ok:
                self.on_step_success(step, res)
            else:
                self.on_step_failure(step, res)

            self.debug(f'Completed step: {step.description}')
        
        # - Extract final results
        if not self.final_results:
            self.final_results = self.extract_final_results()
            self.save()
        return self.final_results

    def plan_validator(self, plan):
        # TODO
        if not isinstance(plan, list):
            return False
        return True

    def invoke_agent(self, input:Input, **kwargs: Any) -> Output:
        self.runnable_config = kwargs.get('config', self.runnable_config)
        if isinstance(input, str):
            self.set_current_goal(input)
        else:
            self.original_input = input
        self.ensure_has_plan()
        return self.execute_plan()

    def get_agent_response(
            self,
            step: AgentPlanStep,
            output_parser: Optional[BaseOutputParser] = None,
            **kw
    ) -> AgentResponse[Output]:
        input = dict(
            current_step = step,
        )
        if self.original_input and isinstance(self.original_input, dict):
            input.update(**self.original_input)
        input.update(**kw)
        res = self.get_single_agent_response(
            input, # Template input args
            output_parser=output_parser,
        )
        return res

    def get_current_step(self):
        if not self.plan:
            return None
        return self.plan.get_current_step()

    def get_last_step_complete_step_attempt(self):
        step = self.get_current_step()
        if not step:
            return None
        if not step.attempts:
            return None
        for a in reversed(step.attempts):
            if a.critic_review:
                return a
            if a.result:
                return a
        return None

    def get_current_step_attempt(self):
        step = self.get_current_step()
        if not step:
            return None
        if not step.attempts:
            return None
        attempt = step.attempts[-1]
        return attempt

    def get_input_vars(self) -> dict:
        vars = super().get_input_vars()
        vars.update(
            **self.get_step_input_vars(self.get_current_step())
        )
        return vars

    def get_step_input_vars(self, step: AgentPlanStep) -> dict:
        past_steps = self.plan.get_past_steps()
        last_attempt = self.get_last_step_complete_step_attempt()
        return dict(
            current_step = step,
            last_attempt = last_attempt,
            step = step,
            plan = self.plan,
            goal = self.goal,
            past_steps = past_steps,
            chat_history = step.get_conversation(last=self.__ATTEMPT_HISTORY_LENGTH__),
        )

    def execute_plan_step(self, step: AgentPlanStep, **kw) -> bool:
        self.save()

        # Fail if we've already tried too many times
        max_attempts = step.max_attempts or self.__MAX_STEP_ATTEMPTS__
        if len(step.attempts) > max_attempts:
            self.add_annotation(
                name='give_up_on_task',
                text=f'Giving up on task due to too many attempts',
                severity='failed',
            )
            raise ValueError('Too many attempts')
        
        # - Start a new attempt at the step
        attempt = step.new_attempt()

        # - Get the LLM's response (may be multiple messages)
        #   This also will update the attempt's conversation
        result = self.execute_step_attempt(step, attempt, **kw)
        attempt.result = result.value
        attempt.conversation = result.chat_messages # TODO put human message before rest for compatibility
        step.save()

        giving_up = False

        if len(result.signals or []) > 0:
            for signal in result.signals:
                if signal == 'ToolGiveUpSignal':
                    self.warn(f'Giving up on step: {step.description}')
                    giving_up = True

        if giving_up:
            self.add_annotation(
                name='give_up_on_task',
                text='Giving up on current task',
                severity='failed',
            )

        if not giving_up and not result.is_success():
            self.warn(f'Failed to execute step: {step.description}')
            self.add_annotation(
                name='task_error',
                text=f'Failed to execute task due to error: `{result.error}`',
                severity='failed',
            )
            return False

        if not giving_up:
            judgement = self.process_step_result(step, attempt)
            if judgement is None:
                self.log_error(f'You must return True or False from `{self.__class__.__name__}.process_step_result`, not None')
                judgement = True

            if not judgement:
                msg = f'Task failed result processing step ({self.__class__.__name__}.process_step_result returned False)'
                self.warn(msg)
                self.add_annotation(
                    name='task_failed_result_processing',
                    text=msg,
                    severity='failed',
                )
                return False
        
        if not giving_up:
            judgement = self.validate_step_result(
                step, attempt, attempt.result
            )
            if judgement is None:
                self.log_error(f'You must return True or False from `{self.__class__.__name__}.validate_step_result`, not None, defaulting to ')
                judgement = True

            if not judgement:
                msg = f'Task failed validation ({self.__class__.__name__}.validate_step_result returned False)'
                self.warn(msg)
                self.add_annotation(
                    name='task_failed_result_validation',
                    text=msg,
                    severity='failed',
                )
                return False


        self.add_annotation(
            name='stage',
            text=f'Extracting Task Result',
            severity='success',
        )
        
        if step.do_extract_context is not False:
            step.final_context = self.extract_step_attempt_context(step, result)
        step.save()
        self.add_annotation(
            name='task_complete',
            text=f'Task completed',
            severity='success',
        )
        return True

    
    def process_step_result(
            self,
            step: AgentPlanStep,
            attempt: AgentPlanStepAttempt
    ) -> bool:
        return True

    def extract_step_attempt_context(
        self,
        step: AgentPlanStep,
        result: AgentResponse
    ) -> str:
        # XXX if we want to support cross model context extraction we would need to normalize the messages to remove tools so other models can understand
        f = self.create_llm_function(
            self.__CONTEXT_EXTRACTOR_PROMPT__ or 
                'planner/generic_describe_task_result.j2',
            MessagesPlaceholder("chat_history"),
            '# Describe Results\nPlease now describe the process, results, and important features.',
            output=str,
            model='gpt-4-turbo',
        )
        chat_history = step.get_conversation(last=self.__ATTEMPT_HISTORY_LENGTH__)
        return f(
            chat_history=chat_history,
            step=step,
        )

    def extract_final_results(self) -> Output:
        res_parser = self.get_final_results_parser()
        if not res_parser:
            res_parser = PlainTextOutputParser()

        f = self.create_llm_function(
            self.__FINAL_RESULTS_EXTRACTOR_PROMPT__ or
                'planner/generic_final_results.j2',
            'Please output the final response for the task',
            output=res_parser,
        )
        past_steps = self.plan.get_past_steps()
        return f(
            goal = self.goal,
            plan = self.plan,
            past_steps = past_steps,
        )


class PlanStepResultCritic(Critic[Input]):
    __SYSTEM_PROMPT_TEMPLATE__ = 'planner/generic_critic.system.j2'
    __USER_PROMPT_TEMPLATE__   = 'planner/generic_critic.user.j2'

    __ATTEMPT_HISTORY_LENGTH__ = 2
    """ Number of past attempts to include in the critic review, 0 for all """

    step_id: Optional[str] = AgentPlanStep.Weak('step')

    def __init__(self, step:AgentPlanStep=None, **kw):
        super().__init__(**kw)
        self.step = step
        self._step = step # Cache the step to prevent lookups

    def is_plaintext_output_parser(self, parser=None) -> bool:
        step: AgentPlanStep = self.step
        parser = parser or step.output_parser

        return super().is_plaintext_parser(parser)

    def review_result(self, result: Input, **kw) -> CriticReview:
        step: AgentPlanStep = self.step
        if not step.do_critic_review:
            return CriticReview(success=True, feedback='No review needed for this step.')

        parsed_result = None
        if self.is_plaintext_output_parser():
            parsed_result = result
        
        return super().review_result(
            result,
            parsed_result=parsed_result,
            step=step,
            chat_history=step.get_conversation(last=self.__ATTEMPT_HISTORY_LENGTH__),
            **kw
        )

    def invoke_agent(self, input: Input, **kwargs: Any) -> CriticReview:
        if isinstance(input, AgentPlanStepAttempt):
            input = input.result
        return self.review_result(input, **kwargs)

from langchain_core.agents import AgentAction, AgentFinish, AgentStep
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder


class CriticalPlanExecutor(PlanExecutor[Input, Output]):
    __CRITIC_CLASS__: PlanStepResultCritic = PlanStepResultCritic

    def critic_approves_step_attempt(
        self,
        step: AgentPlanStep,
        attempt: AgentPlanStepAttempt,
    ) -> bool:
        if not self.__CRITIC_CLASS__:
            self.warn(f'No critic class defined for {self.__class__}, handing out participation trophies...')
            return True

        assert(issubclass(self.__CRITIC_CLASS__, PlanStepResultCritic))
        critic = self.__CRITIC_CLASS__(parent=self, step=step)

        attempt.critic_review = critic.invoke(
            attempt
        )
        step.save()

        if not step.check_if_complete():
            # - If not complete, we will iterate again
            return False
        return True

    def validate_step_result(
            self,
            step: AgentPlanStep,
            attempt: AgentPlanStepAttempt,
            result: Any
    ) -> bool:
        # - Ask the critic if we have completed the step
        if not self.critic_approves_step_attempt(
            step, attempt
        ):
            return False
        return True
